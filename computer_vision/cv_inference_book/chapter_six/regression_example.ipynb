{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d6ead6",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "Por ejemplo considere el caso de querer estimar, usando una variable gaussiana 1D, la distancia al siguiente auto en una ruta contando la cantidad de pixels en su silueta.\n",
    "\n",
    "# Modelo de contingencia de las variables de estado del mundo $w$ sobre los datos $x$\n",
    "\n",
    "$$\n",
    "P(W \\mid X)\n",
    "$$\n",
    "Dado que el model es univarial y continuo, eligimos la variable guassiana 1D, para ello fijamos la  $\\sigma^2$ (varianza) (Porque??) y hacemos que $\\mu$ este en funcion de los datos, asi tenemos dos variables nuevas:\n",
    "$ \\phi_0 + \\phi_1 $\n",
    "y la funcion de probabilidad queda como:\n",
    "$$\n",
    "Pr(w \\mid x, \\theta)\n",
    "= \\mathrm{Norm}_w\\left[ \\phi_0 + \\phi_1 x,\\, \\sigma^2 \\right]\n",
    "$$\n",
    "Por lo que los parametros del modelo son:\n",
    "$$\n",
    "\\theta = \\{ \\phi_0,\\ \\phi_1,\\ \\sigma \\}\n",
    "$$\n",
    "Esta formulacion se conoce como *Linear Regression*\n",
    "El algoritmo de aprendizaje para este modelo enparda $\\{ x_i,\\ w_i \\}_{i=1}^{I}$ los datos del dataset con los parametros del modelo.\n",
    "El enfoque **MAP** seria:\n",
    "$$\n",
    "\\hat{\\theta}\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \\big[ Pr(\\theta \\mid w_{1\\ldots I}, x_{1\\ldots I}) \\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \\big[ Pr(w_{1\\ldots I} \\mid x_{1\\ldots I}, \\theta)\\ Pr(\\theta) \\big]\n",
    "\\quad\\text{(por Bayes)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \n",
    "\\left[\n",
    "\\prod_{i=1}^{I} Pr(w_i \\mid x_i,\\ \\theta)\\ Pr(\\theta)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "# Aclaracion frase del libro\n",
    "\n",
    "\"where we have assumed that the I training pairs {x_i , w_i}_{i=1}^I are independent, and defined a suitable prior Pr(θ).\"\n",
    "\n",
    "A continuación se explica cada parte.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Independencia de los pares (x_i, w_i)\n",
    "\n",
    "El dataset contiene pares:\n",
    "\n",
    "(x_1, w_1), (x_2, w_2), ..., (x_I, w_I)\n",
    "\n",
    "Asumir que son independientes significa que cada observación del dataset no depende de las demás. Esto permite factorizar la probabilidad conjunta de esta forma:\n",
    "\n",
    "Pr(w_1,...,w_I | x_1,...,x_I, θ) = ∏_{i=1}^I Pr(w_i | x_i, θ)\n",
    "\n",
    "Esta suposición se llama i.i.d. (independiente e idénticamente distribuido).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Definir un prior Pr(θ)\n",
    "\n",
    "Los parámetros del modelo son:\n",
    "\n",
    "θ = {φ_0, φ_1, σ}\n",
    "\n",
    "En el enfoque bayesiano, estos parámetros tienen distribuciones previas antes de ver los datos. Por ejemplo:\n",
    "\n",
    "φ_0 ~ N(0, 100)\n",
    "φ_1 ~ N(0, 100)\n",
    "σ ~ HalfNormal(5) -> Significa distribución Half-Normal con escala 5.\n",
    "\n",
    "Este prior Pr(θ) representa nuestras creencias iniciales sobre los parámetros.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cómo se combinan independencia + prior con Bayes\n",
    "\n",
    "Usando la regla de Bayes:\n",
    "\n",
    "Pr(θ | datos) ∝ Pr(datos | θ) * Pr(θ)\n",
    "\n",
    "Y usando independencia:\n",
    "\n",
    "Pr(datos | θ) = ∏_{i=1}^I Pr(w_i | x_i, θ)\n",
    "\n",
    "Esto lleva a la expresión para MAP:\n",
    "\n",
    "θ^ = argmax_θ [ ∏_{i=1}^I Pr(w_i | x_i, θ) * Pr(θ) ]\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "- Asumimos que cada ejemplo del dataset es independiente.\n",
    "- Definimos un prior sobre los parámetros para hacer inferencia bayesiana.\n",
    "- Al combinar prior y likelihood (producto por independencia), obtenemos la ecuación usada para estimación MAP.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dca4b",
   "metadata": {},
   "source": [
    "# A continuacion se va a realizar un ejemplo con la base de datos de Kitti\n",
    "1.- data_object_image_2.zip -> es la base de datos principal\n",
    "2.- data_object_label_2.zip\n",
    "3.- data_object_calib.zip\n",
    "# Estrategia general para el ejercicio\n",
    "1.- Detectar si la imagen contien o no contiene un auto\n",
    "2.- Si contiene extraer la region de la imagen donde  esta el auto\n",
    "3.- Mediante un proceso de background removal extraeer el contorno del auto\n",
    "    - Vamos a usar una opcion mas realista y dura para obtener el contorno del auto una vez detectado, mediante la segmentación dentro del bbox (GrabCut / MaskRCNN / SAM, etc.)\n",
    "    - Hay que validar el proceso anterior (Otra VA que diga si el contorno es valido o no)\n",
    "    - Despues contamos pixeles del resultado\n",
    "4.- Estimar la distancia del vehiculo usando Regression (El algoritmo de inferencia del libro)\n",
    "5.- Que distancia?\n",
    "    Distancia desde el centro óptico de la cámara al centro 3D del auto\n",
    "    Porque eso es exactamente lo que representa z en KITTI Object Detection. Es la profundidad respecto a la cámara izquierda.\n",
    "# Formato del archivo label de KITTI\n",
    "```text\n",
    "0:type\n",
    "1:truncated\n",
    "2:occluded\n",
    "3:alpha\n",
    "4:left\n",
    "5:top\n",
    "6:right\n",
    "7:bottom\n",
    "8:height\n",
    "9:width\n",
    "10:length\n",
    "11:x\n",
    "12:y\n",
    "13:z     <--- distancia en metros\n",
    "14:rotation_y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37157801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "def kitti_pairs_tf_datasets(\n",
    "    root_abs_path,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea tres tf.data.Dataset (train, val, test) con pares (img_path, label_path).\n",
    "\n",
    "    root_abs_path: str\n",
    "        Path absoluto donde existen las carpetas image_2/ y label_2/\n",
    "\n",
    "    test_size: float\n",
    "        Porcentaje de test (0.2 = 20%)\n",
    "\n",
    "    val_size: float\n",
    "        Porcentaje de validation (0.1 = 10%)\n",
    "\n",
    "    Uso típico en Jupyter:\n",
    "        train_ds, val_ds, test_ds = kitti_pairs_tf_datasets(\"/ruta/kitti/training\")\n",
    "    \"\"\"\n",
    "\n",
    "    root = Path(root_abs_path).expanduser().resolve()\n",
    "    img_dir = root / \"image_2\"\n",
    "    label_dir = root / \"label_2\"\n",
    "\n",
    "    # Verificar existencia\n",
    "    if not img_dir.exists():\n",
    "        raise FileNotFoundError(f\"No existe {img_dir}\")\n",
    "    if not label_dir.exists():\n",
    "        raise FileNotFoundError(f\"No existe {label_dir}\")\n",
    "\n",
    "    # Listar imágenes\n",
    "    img_paths = sorted([str(p) for p in img_dir.glob(\"*.png\")])\n",
    "    if len(img_paths) == 0:\n",
    "        raise ValueError(f\"No se encontraron imágenes PNG en {img_dir}\")\n",
    "    print(f\"cantidad de imagenes cargadas: {len(img_paths)}\")\n",
    "    # Emparejar images - labels\n",
    "    pairs = []\n",
    "    for img_path in img_paths:\n",
    "        frame_id = Path(img_path).stem\n",
    "        label_path = label_dir / f\"{frame_id}.txt\"\n",
    "        if label_path.exists():\n",
    "            pairs.append((img_path, str(label_path)))\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(\"No se encontraron pares imagen-label\")\n",
    "\n",
    "    # Convertir a tensores\n",
    "    img_tensor = tf.constant([p[0] for p in pairs], dtype=tf.string)\n",
    "    lab_tensor = tf.constant([p[1] for p in pairs], dtype=tf.string)\n",
    "\n",
    "    n = tf.shape(img_tensor)[0]\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        idx = tf.random.shuffle(tf.range(n), seed=seed)\n",
    "        img_tensor = tf.gather(img_tensor, idx)\n",
    "        lab_tensor = tf.gather(lab_tensor, idx)\n",
    "\n",
    "    # Calcular tamaños\n",
    "    n_test = int(n.numpy() * test_size)\n",
    "    n_val  = int(n.numpy() * val_size)\n",
    "    n_train = n.numpy() - n_test - n_val\n",
    "\n",
    "    # Particiones\n",
    "    train_imgs = img_tensor[:n_train]\n",
    "    train_labs = lab_tensor[:n_train]\n",
    "\n",
    "    val_imgs = img_tensor[n_train:n_train + n_val]\n",
    "    val_labs = lab_tensor[n_train:n_train + n_val]\n",
    "\n",
    "    test_imgs = img_tensor[n_train + n_val:]\n",
    "    test_labs = lab_tensor[n_train + n_val:]\n",
    "\n",
    "    # tf.data.Dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_imgs, train_labs))\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((val_imgs, val_labs))\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((test_imgs, test_labs))\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16228a9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics_learning_path",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
