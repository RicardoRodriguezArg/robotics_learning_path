{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d6ead6",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "Por ejemplo considere el caso de querer estimar, usando una variable gaussiana 1D, la distancia al siguiente auto en una ruta contando la cantidad de pixels en su silueta.\n",
    "\n",
    "# Modelo de contingencia de las variables de estado del mundo $w$ sobre los datos $x$\n",
    "\n",
    "$$\n",
    "P(W \\mid X)\n",
    "$$\n",
    "Dado que el model es univarial y continuo, eligimos la variable guassiana 1D, para ello fijamos la  $\\sigma^2$ (varianza) (Porque??) y hacemos que $\\mu$ este en funcion de los datos, asi tenemos dos variables nuevas:\n",
    "$ \\phi_0 + \\phi_1 $\n",
    "y la funcion de probabilidad queda como:\n",
    "$$\n",
    "Pr(w \\mid x, \\theta)\n",
    "= \\mathrm{Norm}_w\\left[ \\phi_0 + \\phi_1 x,\\, \\sigma^2 \\right]\n",
    "$$\n",
    "Por lo que los parametros del modelo son:\n",
    "$$\n",
    "\\theta = \\{ \\phi_0,\\ \\phi_1,\\ \\sigma \\}\n",
    "$$\n",
    "Esta formulacion se conoce como *Linear Regression*\n",
    "El algoritmo de aprendizaje para este modelo enparda $\\{ x_i,\\ w_i \\}_{i=1}^{I}$ los datos del dataset con los parametros del modelo.\n",
    "El enfoque **MAP** seria:\n",
    "$$\n",
    "\\hat{\\theta}\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \\big[ Pr(\\theta \\mid w_{1\\ldots I}, x_{1\\ldots I}) \\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \\big[ Pr(w_{1\\ldots I} \\mid x_{1\\ldots I}, \\theta)\\ Pr(\\theta) \\big]\n",
    "\\quad\\text{(por Bayes)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\underset{\\theta}{\\arg\\max}\\ \n",
    "\\left[\n",
    "\\prod_{i=1}^{I} Pr(w_i \\mid x_i,\\ \\theta)\\ Pr(\\theta)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "# Aclaracion frase del libro\n",
    "\n",
    "\"where we have assumed that the I training pairs {x_i , w_i}_{i=1}^I are independent, and defined a suitable prior Pr(θ).\"\n",
    "\n",
    "A continuación se explica cada parte.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Independencia de los pares (x_i, w_i)\n",
    "\n",
    "El dataset contiene pares:\n",
    "\n",
    "(x_1, w_1), (x_2, w_2), ..., (x_I, w_I)\n",
    "\n",
    "Asumir que son independientes significa que cada observación del dataset no depende de las demás. Esto permite factorizar la probabilidad conjunta de esta forma:\n",
    "\n",
    "Pr(w_1,...,w_I | x_1,...,x_I, θ) = ∏_{i=1}^I Pr(w_i | x_i, θ)\n",
    "\n",
    "Esta suposición se llama i.i.d. (independiente e idénticamente distribuido).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Definir un prior Pr(θ)\n",
    "\n",
    "Los parámetros del modelo son:\n",
    "\n",
    "θ = {φ_0, φ_1, σ}\n",
    "\n",
    "En el enfoque bayesiano, estos parámetros tienen distribuciones previas antes de ver los datos. Por ejemplo:\n",
    "\n",
    "φ_0 ~ N(0, 100)\n",
    "φ_1 ~ N(0, 100)\n",
    "σ ~ HalfNormal(5) -> Significa distribución Half-Normal con escala 5.\n",
    "\n",
    "Este prior Pr(θ) representa nuestras creencias iniciales sobre los parámetros.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cómo se combinan independencia + prior con Bayes\n",
    "\n",
    "Usando la regla de Bayes:\n",
    "\n",
    "Pr(θ | datos) ∝ Pr(datos | θ) * Pr(θ)\n",
    "\n",
    "Y usando independencia:\n",
    "\n",
    "Pr(datos | θ) = ∏_{i=1}^I Pr(w_i | x_i, θ)\n",
    "\n",
    "Esto lleva a la expresión para MAP:\n",
    "\n",
    "θ^ = argmax_θ [ ∏_{i=1}^I Pr(w_i | x_i, θ) * Pr(θ) ]\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "- Asumimos que cada ejemplo del dataset es independiente.\n",
    "- Definimos un prior sobre los parámetros para hacer inferencia bayesiana.\n",
    "- Al combinar prior y likelihood (producto por independencia), obtenemos la ecuación usada para estimación MAP.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dca4b",
   "metadata": {},
   "source": [
    "# A continuacion se va a realizar un ejemplo con la base de datos de Kitti\n",
    "1.- data_object_image_2.zip -> es la base de datos principal\n",
    "2.- data_object_label_2.zip\n",
    "3.- data_object_calib.zip\n",
    "# Estrategia general para el ejercicio\n",
    "1.- Detectar si la imagen contien o no contiene un auto\n",
    "2.- Si contiene extraer la region de la imagen donde  esta el auto\n",
    "3.- Mediante un proceso de background removal extraeer el contorno del auto\n",
    "    - Vamos a usar una opcion mas realista y dura para obtener el contorno del auto una vez detectado, mediante la segmentación dentro del bbox (GrabCut / MaskRCNN / SAM, etc.)\n",
    "    - Hay que validar el proceso anterior (Otra VA que diga si el contorno es valido o no)\n",
    "    - Despues contamos pixeles del resultado\n",
    "4.- Estimar la distancia del vehiculo usando Regression (El algoritmo de inferencia del libro)\n",
    "5.- Que distancia?\n",
    "    Distancia desde el centro óptico de la cámara al centro 3D del auto\n",
    "    Porque eso es exactamente lo que representa z en KITTI Object Detection. Es la profundidad respecto a la cámara izquierda.\n",
    "# Formato del archivo label de KITTI\n",
    "```text\n",
    "0:type\n",
    "1:truncated\n",
    "2:occluded\n",
    "3:alpha\n",
    "4:left\n",
    "5:top\n",
    "6:right\n",
    "7:bottom\n",
    "8:height\n",
    "9:width\n",
    "10:length\n",
    "11:x\n",
    "12:y\n",
    "13:z     <--- distancia en metros\n",
    "14:rotation_y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fddd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37157801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "def kitti_pairs_tf_datasets(\n",
    "    root_abs_path,\n",
    "    test_size=0.2,\n",
    "    val_size=0.1,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea tres tf.data.Dataset (train, val, test) con pares (img_path, label_path).\n",
    "\n",
    "    root_abs_path: str\n",
    "        Path absoluto donde existen las carpetas image_2/ y label_2/\n",
    "\n",
    "    test_size: float\n",
    "        Porcentaje de test (0.2 = 20%)\n",
    "\n",
    "    val_size: float\n",
    "        Porcentaje de validation (0.1 = 10%)\n",
    "\n",
    "    Uso típico en Jupyter:\n",
    "        train_ds, val_ds, test_ds = kitti_pairs_tf_datasets(\"/ruta/kitti/training\")\n",
    "    \"\"\"\n",
    "\n",
    "    root = Path(root_abs_path).expanduser().resolve()\n",
    "    img_dir = root / \"image_2\"\n",
    "    label_dir = root / \"label_2\"\n",
    "\n",
    "    # Verificar existencia\n",
    "    if not img_dir.exists():\n",
    "        raise FileNotFoundError(f\"No existe {img_dir}\")\n",
    "    if not label_dir.exists():\n",
    "        raise FileNotFoundError(f\"No existe {label_dir}\")\n",
    "\n",
    "    # Listar imágenes\n",
    "    img_paths = sorted([str(p) for p in img_dir.glob(\"*.png\")])\n",
    "    if len(img_paths) == 0:\n",
    "        raise ValueError(f\"No se encontraron imágenes PNG en {img_dir}\")\n",
    "    print(f\"cantidad de imagenes cargadas: {len(img_paths)}\")\n",
    "    # Emparejar images - labels\n",
    "    pairs = []\n",
    "    for img_path in img_paths:\n",
    "        frame_id = Path(img_path).stem\n",
    "        label_path = label_dir / f\"{frame_id}.txt\"\n",
    "        if label_path.exists():\n",
    "            pairs.append((img_path, str(label_path)))\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(\"No se encontraron pares imagen-label\")\n",
    "\n",
    "    # Convertir a tensores\n",
    "    img_tensor = tf.constant([p[0] for p in pairs], dtype=tf.string)\n",
    "    lab_tensor = tf.constant([p[1] for p in pairs], dtype=tf.string)\n",
    "\n",
    "    n = tf.shape(img_tensor)[0]\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        idx = tf.random.shuffle(tf.range(n), seed=seed)\n",
    "        img_tensor = tf.gather(img_tensor, idx)\n",
    "        lab_tensor = tf.gather(lab_tensor, idx)\n",
    "\n",
    "    # Calcular tamaños\n",
    "    n_test = int(n.numpy() * test_size)\n",
    "    n_val  = int(n.numpy() * val_size)\n",
    "    n_train = n.numpy() - n_test - n_val\n",
    "\n",
    "    # Particiones\n",
    "    train_imgs = img_tensor[:n_train]\n",
    "    train_labs = lab_tensor[:n_train]\n",
    "\n",
    "    val_imgs = img_tensor[n_train:n_train + n_val]\n",
    "    val_labs = lab_tensor[n_train:n_train + n_val]\n",
    "\n",
    "    test_imgs = img_tensor[n_train + n_val:]\n",
    "    test_labs = lab_tensor[n_train + n_val:]\n",
    "\n",
    "    # tf.data.Dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_imgs, train_labs))\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((val_imgs, val_labs))\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((test_imgs, test_labs))\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b128a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de imagenes cargadas: 7481\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = kitti_pairs_tf_datasets(\"/home/operador/Documents/datasets/trainning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f458d",
   "metadata": {},
   "source": [
    "Bien ahora sieguiendo con el libro, necesito crear un \"detector de autos\" pero siguiendo el ejercicio del libro:, por lo tanto es un problema de clasificacion y vamos a usar la distribucion categorical.\n",
    "Como algorithmo de aprendizaje vamos a usar Maximum Likelihood primero.\n",
    "Porque Para hacer la regression necesito saber si estoy lidiando con un auto o no, extraer los features que me pide el ejercicio (solo disponible si hay un auto , ademas debo saber donde esta, eso lo puedo aprender con el dataset que me dieron.) y recien ahi hacer la regresssion. Por eso que este ejercicio a pesar de que se trata de una regression tiene pasos escondididos que no son \"puros\" del ejericicio sino accesrios.\n",
    "Para ello necesito extraer los siguientes features:\n",
    "- área del contorno\n",
    "- relación de aspecto\n",
    "- ancho y alto\n",
    "- perímetro\n",
    "- compacidad\n",
    "- posición del bounding box\n",
    "Solo usando lo que se aprendio hasta el capitulo 6 (sin deep learning, CNN, HOG u otros metodos).\n",
    "Como es *clasificacion Binaria* usamos Bernoulli:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ddfa7",
   "metadata": {},
   "source": [
    "\n",
    "# Modelo matemático para detección binaria de autos (regresión logística)\n",
    "\n",
    "## 1. Notación\n",
    "\n",
    "En KITTI, cada objeto anotado tiene un bounding box 2D definido por  \n",
    "$(x_1, y_1, x_2, y_2)$.\n",
    "\n",
    "A partir de ese bounding box definimos los **features**:\n",
    "\n",
    "- $w_{bb} = x_2 - x_1$  (ancho del bounding box)\n",
    "- $h_{bb} = y_2 - y_1$  (alto)\n",
    "- $a_{bb} = w_{bb}\\,h_{bb}$ (área)\n",
    "- $r_{bb} = \\dfrac{w_{bb}}{h_{bb}}$ (aspect ratio)\n",
    "\n",
    "El vector de entrada es entonces:\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "w_{bb} \\\\\n",
    "h_{bb} \\\\\n",
    "a_{bb} \\\\\n",
    "r_{bb}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^4\n",
    "$$\n",
    "\n",
    "La etiqueta binaria se define como:\n",
    "\n",
    "- $t = 1$ si el objeto es un **Car**\n",
    "- $t = 0$ si es **Pedestrian**, **Cyclist**, u otro tipo\n",
    "\n",
    "Los parámetros del modelo son:\n",
    "\n",
    "- $w \\in \\mathbb{R}^4$\n",
    "- $b \\in \\mathbb{R}$\n",
    "\n",
    "Nuestro dataset es:\n",
    "\n",
    "$$\n",
    "\\{ (x_n, t_n) \\}_{n=1}^N\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Modelo probabilístico (Bernoulli + regresión logística)\n",
    "\n",
    "Queremos modelar:\n",
    "\n",
    "$$\n",
    "p(t = 1 \\mid x, w, b) = y(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(t = 0 \\mid x, w, b) = 1 - y(x)\n",
    "$$\n",
    "\n",
    "Modelo lineal + sigmoide:\n",
    "\n",
    "$$\n",
    "a = w^\\top x + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "y(x) = \\sigma(a) = \\frac{1}{1 + \\exp(-a)}\n",
    "$$\n",
    "\n",
    "Distribución Bernoulli:\n",
    "\n",
    "$$\n",
    "p(t \\mid x, w, b) = y(x)^t \\, (1 - y(x))^{1 - t}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Verosimilitud y log-verosimilitud\n",
    "\n",
    "Asumiendo datos independientes:\n",
    "\n",
    "$$\n",
    "L(w,b) = \\prod_{n=1}^N p(t_n \\mid x_n, w,b)\n",
    "= \\prod_{n=1}^N y_n^{t_n} (1 - y_n)^{1 - t_n}\n",
    "$$\n",
    "\n",
    "donde $y_n = \\sigma(w^\\top x_n + b)$.\n",
    "\n",
    "Log-verosimilitud:\n",
    "\n",
    "$$\n",
    "\\ell(w,b) =\n",
    "\\sum_{n=1}^N \\Big[ t_n \\log(y_n) + (1 - t_n)\\log(1 - y_n) \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Por qué necesitamos el gradiente\n",
    "\n",
    "La estimación por ML consiste en:\n",
    "\n",
    "$$\n",
    "(\\hat{w}, \\hat{b}) = \\arg\\max_{w,b}\\, \\ell(w,b)\n",
    "$$\n",
    "\n",
    "Para aplicar métodos de optimización (como gradient ascent) necesitamos los gradientes de $\\ell$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Derivación del gradiente\n",
    "\n",
    "Resultado final para cada muestra:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\ell_n(w,b) = (t_n - y_n)\\, x_n\n",
    "$$\n",
    "\n",
    "Sumando sobre todas:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\ell(w,b) = \\sum_{n=1}^N (t_n - y_n)\\, x_n\n",
    "$$\n",
    "\n",
    "Gradiente respecto de $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell(w,b)}{\\partial b} =\n",
    "\\sum_{n=1}^N (t_n - y_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Método de aprendizaje e inferencia\n",
    "\n",
    "### Nombre del método\n",
    "\n",
    "**Regresión logística binaria con máxima verosimilitud**, optimizada mediante **gradient ascent**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1. Actualización de parámetros\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} + \\eta \\sum_{n=1}^N (t_n - y_n) x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{(k+1)} = b^{(k)} + \\eta \\sum_{n=1}^N (t_n - y_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2. Inferencia\n",
    "\n",
    "Dado un nuevo vector de características `x_new`:\n",
    "\n",
    "```\n",
    "y_new = sigma( w_hat^T x_new + b_hat )\n",
    "```\n",
    "\n",
    "Regla de decisión:\n",
    "\n",
    "- Si `y_new ≥ 0.5` → hay auto  \n",
    "- Si `y_new < 0.5` → no hay auto\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Resumen\n",
    "\n",
    "- Modelo: regresión logística binaria  \n",
    "- Distribución: Bernoulli  \n",
    "- Aprendizaje: máxima verosimilitud  \n",
    "- Gradiente:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\ell(w,b) = \\sum_{n=1}^N (t_n - y_n)x_n\n",
    "$$\n",
    "\n",
    "- Inferencia: $y_{\\text{new}} = \\sigma(\\hat{w}^\\top x_{\\text{new}} + \\hat{b})$ con umbral 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30f308",
   "metadata": {},
   "source": [
    "Aclaracion:\n",
    "```\n",
    "statistical model like logistic regression, which predicts the probability of a categorical outcome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16228a9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd177d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [9.8329956e+01 1.6492001e+02 1.6216578e+04 5.9622818e-01]\n",
      "p(t=1 | x) = 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ============================================\n",
    "# 1) Extracción de features del bounding box\n",
    "#    x = [w_bb, h_bb, area, aspect]\n",
    "# ============================================\n",
    "\n",
    "def kitti_bbox_features(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 pueden ser escalares o tensores (por ejemplo (N,)).\n",
    "\n",
    "    Devuelve un tensor de shape (..., 4) con:\n",
    "        [width, height, area, aspect_ratio]\n",
    "    \"\"\"\n",
    "    x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n",
    "    y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n",
    "    x2 = tf.convert_to_tensor(x2, dtype=tf.float32)\n",
    "    y2 = tf.convert_to_tensor(y2, dtype=tf.float32)\n",
    "\n",
    "    width  = x2 - x1          # w_bb\n",
    "    height = y2 - y1          # h_bb\n",
    "    area   = width * height   # a_bb\n",
    "\n",
    "    # aspect ratio = width / height (evitar división por cero)\n",
    "    eps = tf.constant(1e-6, dtype=tf.float32)\n",
    "    aspect = width / (height + eps)\n",
    "\n",
    "    # stack en el último eje: (..., 4)\n",
    "    features = tf.stack([width, height, area, aspect], axis=-1)\n",
    "    return features\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2) Modelo logístico: y(x) = sigma(w^T x + b)\n",
    "# ============================================\n",
    "\n",
    "class LogisticCarDetector(tf.Module):\n",
    "    \"\"\"\n",
    "    Modelo logístico binario para 'Car' vs 'no-Car'\n",
    "    usando como entrada x ∈ R^4 (features del bbox).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=4, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Parámetros del modelo:\n",
    "        # w ∈ R^{D x 1}, b ∈ R\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.normal(shape=(input_dim, 1), stddev=0.1),\n",
    "            name=\"w\"\n",
    "        )\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros(shape=(1,)),\n",
    "            name=\"b\"\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        X: tensor de shape (N, D) o (D,) con features:\n",
    "           [w_bb, h_bb, area, aspect]\n",
    "\n",
    "        Devuelve:\n",
    "            y: probabilidad p(t=1 | x) con shape (N,)\n",
    "        \"\"\"\n",
    "        X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "        # Asegurar shape (N, D)\n",
    "        if len(X.shape) == 1:\n",
    "            X = tf.expand_dims(X, axis=0)\n",
    "\n",
    "        # logits = X w + b  → shape (N, 1)\n",
    "        logits = tf.matmul(X, self.w) + self.b\n",
    "\n",
    "        # y = sigma(logits) ∈ (0,1)\n",
    "        y = tf.sigmoid(logits)  # shape (N, 1)\n",
    "\n",
    "        # Devolver como vector (N,)\n",
    "        return tf.squeeze(y, axis=-1)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3) Ejemplo de uso solo del modelo matemático\n",
    "#    (sin entrenamiento todavía)\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo: un solo bounding box\n",
    "    x1, y1, x2, y2 = 712.40, 143.00, 810.73, 307.92\n",
    "\n",
    "    # 1) Construir features x = [w_bb, h_bb, area, aspect]\n",
    "    x_feat = kitti_bbox_features(x1, y1, x2, y2)  # shape (4,)\n",
    "\n",
    "    # 2) Crear el modelo\n",
    "    model = LogisticCarDetector(input_dim=4)\n",
    "\n",
    "    # 3) Forward: probabilidad de que esto sea un \"Car\"\n",
    "    y_prob = model(x_feat)  # shape (), probabilidad en [0,1]\n",
    "\n",
    "    print(\"Features:\", x_feat.numpy())\n",
    "    print(\"p(t=1 | x) =\", float(y_prob))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics_learning_path",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
